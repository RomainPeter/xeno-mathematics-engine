#!/usr/bin/env python3
"""
Build Public Benchmark Pack
Creates a zip file with summary.json, seeds, merkle.txt, sbom.json, reproduce.md
"""

import argparse
import hashlib
import json
import sys
import time
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Optional

# Add current directory to Python path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))
from pefc.config.loader import get_config
from pefc.events import get_event_bus
from pefc.events import topics as E
from pefc.logging import get_logger, set_context, setup_logging_from_config
from pefc.metrics.build_provider import build_provider
from pefc.pack.merkle import build_entries, build_manifest, compute_merkle_root
from pefc.pack.zipper import ZipAdder
from pefc.runner import run_pack_build
from pefc.summary import build_summary

# Initialize logging (will be reconfigured with config)
logger = get_logger(__name__)

# Setup event bus with logging subscriber
bus = get_event_bus()
try:
    from pefc.events.subscribers import LoggingSubscriber

    bus.subscribe("*", LoggingSubscriber(logger).handler, priority=-100)
except Exception as e:
    import logging
    logging.warning(f"Failed to subscribe to event bus: {e}")


class PublicBenchPackBuilder:
    """Builds public benchmark pack with all necessary components."""

    def __init__(self, output_dir: Optional[Path] = None, pack_name: Optional[str] = None):
        self.output_dir = Path(output_dir) if output_dir else Path("artifacts")
        self.output_dir.mkdir(exist_ok=True)
        self.pack_name = pack_name or "bench_pack_v0.1.0"
        self.pack_path = self.output_dir / f"{self.pack_name}.zip"

    def build_summary(
        self,
        metrics_sources: list[str],
        include_aggregates: bool = False,
        weight_key: str = "n_items",
        dedup: str = "first",
        validate: bool = False,
        bounded_metrics: Optional[list[str]] = None,
        version: str = "v0.1.0",
        provider=None,
    ) -> dict:
        """Build summary.json with aggregated metrics using new API."""
        logger.info("Building summary.json", extra={"event": "summary.build.start"})

        # Convert string paths to Path objects
        sources = [Path(p) for p in metrics_sources]

        # Use new build_summary API
        summary_path = self.output_dir / "summary.json"
        result = build_summary(
            sources=sources,
            out_path=summary_path,
            include_aggregates=include_aggregates,
            weight_key=weight_key,
            dedup=dedup,
            version=version,
            validate=validate,
            bounded_metrics=bounded_metrics,
            schema_path=Path("schema/summary.schema.json"),
            provider=provider,
        )

        # Add legacy fields for backward compatibility
        result["generated_at"] = datetime.now().isoformat()
        result["reproducibility"] = {
            "deterministic": True,
            "seeds_provided": True,
            "merkle_attestation": True,
        }

        # Emit metrics summary built event
        bus.emit(
            E.METRICS_SUMMARY_BUILT,
            out_path=str(summary_path),
            runs=len(result.get("runs", [])),
            version=version,
        )

        return result

    def collect_seeds(self) -> list:
        """Collect deterministic seeds for reproduction."""
        logger.info("Collecting seeds", extra={"event": "seeds.collect.start"})

        seeds = []

        # Look for seed files
        seed_sources = ["out/seeds.json", "artifacts/seeds.json", "configs/seeds.json"]

        for source in seed_sources:
            if Path(source).exists():
                with open(source) as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        seeds.extend(data)
                    elif isinstance(data, dict) and "seeds" in data:
                        seeds.extend(data["seeds"])

        # If no seeds found, generate default seeds
        if not seeds:
            seeds = [42, 123, 456, 789, 999]

        return seeds

    def get_merkle_root(self) -> str:
        """Get Merkle root for integrity verification."""
        print("ðŸ”— Getting Merkle root...")

        merkle_file = Path("out/journal/merkle.txt")
        if merkle_file.exists():
            return merkle_file.read_text().strip()

        # Generate Merkle root from current state
        return self.generate_merkle_root()

    def generate_merkle_root(self) -> str:
        """Generate Merkle root from current state."""
        # Collect all relevant files
        files_to_hash = []

        # Add metrics files
        for pattern in ["out/metrics.json", "artifacts/*/metrics*.json"]:
            for file_path in Path(".").glob(pattern):
                if file_path.is_file():
                    files_to_hash.append(file_path)

        # Add configuration files
        for pattern in ["configs/*.json", "schemas/*.json"]:
            for file_path in Path(".").glob(pattern):
                if file_path.is_file():
                    files_to_hash.append(file_path)

        # Calculate Merkle root
        if files_to_hash:
            # Sort files for deterministic ordering
            files_to_hash.sort()

            # Calculate combined hash
            combined_content = ""
            for file_path in files_to_hash:
                with open(file_path, "rb") as f:
                    combined_content += f.read().decode("utf-8", errors="ignore")

            merkle_root = hashlib.sha256(combined_content.encode()).hexdigest()
        else:
            # Default Merkle root if no files found
            merkle_root = hashlib.sha256(b"discovery-engine-2cat-v0.1.0").hexdigest()

        return merkle_root

    def get_sbom(self) -> dict:
        """Get Software Bill of Materials."""
        logger.info("Getting SBOM", extra={"event": "sbom.collect.start"})

        sbom_file = Path("out/sbom.json")
        if sbom_file.exists():
            with open(sbom_file) as f:
                return json.load(f)

        # Generate basic SBOM
        return {
            "version": "v0.1.0",
            "generated_at": datetime.now().isoformat(),
            "packages": [],
            "vulnerabilities": [],
            "summary": {
                "total_packages": 0,
                "high_vulnerabilities": 0,
                "critical_vulnerabilities": 0,
            },
        }

    def generate_reproduce_instructions(self) -> str:
        """Generate reproduction instructions."""
        logger.info(
            "Generating reproduction instructions",
            extra={"event": "reproduce.generate.start"},
        )

        instructions = """# Discovery Engine 2â€‘Cat â€” Reproduction Instructions

## Prerequisites

- Python 3.11+
- Docker 24.0+
- Git

## Setup

1. Clone the repository:
```bash
git clone https://github.com/your-org/discovery-engine-2cat.git
cd discovery-engine-2cat
```

2. Create virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

## Reproduction

1. Set deterministic seeds:
```bash
export DISCOVERY_SEED=42
export PYTHONHASHSEED=42
```

2. Run benchmark:
```bash
make regtech-demo
```

3. Verify results:
```bash
make determinism
```

4. Check metrics:
```bash
cat out/metrics.json
```

## Verification

### Merkle Root
The Merkle root should match the provided value:
```bash
cat out/journal/merkle.txt
```

### SBOM
The SBOM should show 0 High/Critical vulnerabilities:
```bash
cat out/sbom.json
```

### Metrics
Expected metrics ranges:
- Coverage gain: 0.18-0.25
- Novelty average: 0.19-0.22
- Audit cost p95: 950-1000ms

## Troubleshooting

### Common Issues
1. **Seed mismatch**: Ensure PYTHONHASHSEED is set
2. **Docker issues**: Ensure Docker is running
3. **Permission issues**: Check file permissions

### Support
- GitHub Issues: https://github.com/your-org/discovery-engine-2cat/issues
- Email: contact@your-org.com

## License

This benchmark pack is released under the MIT License.
"""

        return instructions

    def build_pack(
        self,
        metrics_sources: list[str],
        include_aggregates: bool = False,
        weight_key: str = "n_items",
        dedup: str = "first",
        validate: bool = False,
        bounded_metrics: Optional[list[str]] = None,
        version: str = "v0.1.0",
    ) -> bool:
        """Build the complete benchmark pack."""
        logger.info("Building public benchmark pack", extra={"event": "pack.build.start"})

        try:
            # 1) Build payload files list (exclude manifest.json and merkle.txt from Merkle calculation)
            payload_files = []

            # Core files that will be included in Merkle calculation
            summary = self.build_summary(
                metrics_sources,
                include_aggregates,
                weight_key,
                dedup,
                validate,
                bounded_metrics,
                version,
            )
            summary_path = self.output_dir / "summary.json"
            summary_path.write_text(json.dumps(summary, indent=2), encoding="utf-8")
            payload_files.append((summary_path, "summary.json"))

            # Seeds
            seeds = self.collect_seeds()
            seeds_path = self.output_dir / "seeds.json"
            seeds_path.write_text(json.dumps(seeds, indent=2), encoding="utf-8")
            payload_files.append((seeds_path, "seeds.json"))

            # SBOM
            sbom = self.get_sbom()
            sbom_path = self.output_dir / "sbom.json"
            sbom_path.write_text(json.dumps(sbom, indent=2), encoding="utf-8")
            payload_files.append((sbom_path, "sbom.json"))

            # Reproduction instructions
            reproduce_instructions = self.generate_reproduce_instructions()
            reproduce_path = self.output_dir / "reproduce.md"
            reproduce_path.write_text(reproduce_instructions, encoding="utf-8")
            payload_files.append((reproduce_path, "reproduce.md"))

            # Additional files (if they exist)
            additional_files_raw = [
                "out/metrics.json",
                "configs/llm.yaml",
                "schemas/examples/regtech_domain_spec.overrides.json",
            ]

            for file_path in additional_files_raw:
                if Path(file_path).exists():
                    payload_files.append((Path(file_path), Path(file_path).name))

            # 2) Build Merkle entries and compute root
            entries = build_entries(payload_files)
            merkle_root = compute_merkle_root(entries)

            # 3) Build manifest (excluded from Merkle calculation)
            manifest_obj = build_manifest(entries, merkle_root, version)
            manifest_json = json.dumps(manifest_obj, ensure_ascii=False, indent=2)

            # 4) Create zip with ZipAdder for deduplication
            adder = ZipAdder()
            with zipfile.ZipFile(self.pack_path, "w", zipfile.ZIP_DEFLATED) as zipf:
                # Write payload files
                for entry in entries:
                    adder.add_file(zipf, entry.src_path, entry.arcname)

                # Write proof artifacts (excluded from Merkle calculation)
                adder.add_text(zipf, "manifest.json", manifest_json)
                adder.add_text(zipf, "merkle.txt", merkle_root + "\n")

            # Log final list of files
            logger.info(
                "Files written to zip",
                extra={
                    "event": "zip.complete",
                    "kv": {"file_count": len(adder.seen), "files": sorted(adder.seen)},
                },
            )
            logger.info(
                "Built benchmark pack",
                extra={
                    "event": "pack.build.complete",
                    "kv": {"pack_path": str(self.pack_path)},
                },
            )

            # Emit pack built event
            bus.emit(
                E.PACK_BUILT,
                zip=str(self.pack_path),
                merkle_root=merkle_root,
                manifest=True,
            )

            # Calculate and display pack info
            pack_size = self.pack_path.stat().st_size
            logger.info(
                "Pack info",
                extra={
                    "event": "pack.info",
                    "kv": {
                        "size_mb": round(pack_size / 1024 / 1024, 2),
                        "file_count": len(adder.seen),
                    },
                },
            )

            # Verify pack integrity
            with zipfile.ZipFile(self.pack_path, "r") as zipf:
                file_list = zipf.namelist()
                logger.info(
                    "Pack contents",
                    extra={"event": "pack.contents", "kv": {"files": file_list}},
                )

            return True

        except Exception as e:
            logger.error(
                "Failed to build benchmark pack",
                extra={"event": "pack.build.error", "kv": {"error": str(e)}},
                exc_info=True,
            )
            return False

    def sign_pack(self) -> bool:
        """Sign the benchmark pack for integrity verification."""
        logger.info("Signing benchmark pack", extra={"event": "pack.sign.start"})

        try:
            # Generate signature
            with open(self.pack_path, "rb") as f:
                content = f.read()
                signature = hashlib.sha256(content).hexdigest()

            # Save signature
            signature_file = self.output_dir / f"{self.pack_name}.sig"
            with open(signature_file, "w") as f:
                f.write(signature)

            logger.info(
                "Pack signed",
                extra={
                    "event": "pack.sign.complete",
                    "kv": {"sig_path": str(signature_file), "signature": signature},
                },
            )

            # Emit sign success event
            bus.emit(
                E.SIGN_OK,
                zip=str(self.pack_path),
                sig=str(signature_file),
                provider="sha256",
            )
            return True

        except Exception as e:
            logger.error(
                "Failed to sign pack",
                extra={"event": "pack.sign.error", "kv": {"error": str(e)}},
                exc_info=True,
            )

            # Emit sign failure event
            bus.emit(
                E.SIGN_FAIL,
                zip=str(self.pack_path),
                error=str(e),
                provider="sha256",
            )
            return False


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description="Build Public Benchmark Pack",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/build_public_bench_pack.py
  python scripts/build_public_bench_pack.py --config config/pack.yaml
  python scripts/build_public_bench_pack.py --allow-partial
  python scripts/build_public_bench_pack.py --partial-is-success
        """,
    )

    parser.add_argument("--config", help="Fichier de configuration YAML (dÃ©faut: config/pack.yaml)")
    parser.add_argument(
        "--allow-partial",
        action="store_true",
        help="Ne pas Ã©chouer si Ã©tape optionnelle Ã©choue (status PARTIAL -> code 10)",
    )
    parser.add_argument(
        "--partial-is-success",
        action="store_true",
        help="Mappe PARTIAL sur code 0 (succÃ¨s logique)",
    )
    parser.add_argument(
        "--json-logs",
        action="store_true",
        help="Activer les logs JSON",
    )

    args = parser.parse_args()

    # Load configuration
    config_path = Path(args.config) if args.config else None
    config = get_config(config_path)

    # Setup logging from config
    setup_logging_from_config(config)
    logger = get_logger(__name__)

    logger.info(
        "Building Discovery Engine 2â€‘Cat Public Benchmark Pack",
        extra={"event": "pack.init"},
    )

    # Set context
    set_context(
        pack_version=config.pack.version,
        pack_name=config.pack.pack_name,
        run_id=f"build_{int(time.time())}",
    )

    logger.info("Starting pack build", extra={"event": "pack.start"})
    logger.info(
        "Using config",
        extra={"kv": {"config_path": str(config_path) if config_path else "default"}},
    )
    if hasattr(config, "_base_dir"):
        logger.info("Base directory", extra={"kv": {"base_dir": str(config._base_dir)}})

    # Build provider if configured
    provider = None
    if getattr(config.metrics, "provider", None):
        provider = build_provider(config)
        logger.info("Using metrics provider", extra={"provider": provider.describe()})

    # Run pack build with error handling
    result = run_pack_build(
        config_path=str(config_path) if config_path else None,
        allow_partial=args.allow_partial or args.partial_is_success,
        partial_is_success=args.partial_is_success,
    )

    # Log final result
    logger.info(
        "Pack build completed",
        extra={
            "event": "pack.complete",
            "kv": {
                "status": result.status,
                "exit_code": result.exit_code,
                "artifacts": result.artifacts,
                "reasons": result.reasons,
                "errors": result.errors,
            },
        },
    )

    # Print machine-readable result to stdout
    print(
        json.dumps(
            {
                "status": result.status,
                "exit_code": result.exit_code,
                "artifacts": result.artifacts,
                "reasons": result.reasons,
                "errors": result.errors,
            },
            ensure_ascii=False,
        )
    )

    # Exit with appropriate code
    sys.exit(result.exit_code)


if __name__ == "__main__":
    main()
