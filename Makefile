PY=python3

.PHONY: setup verify demo audit-pack logs release schema-test validate fmt demo-s1 deps-lock build-verifier-pinned audit 2cat-shadow 2cat-active s2-bench 2cat-report validate-summary

setup:
	$(PY) -m venv .venv && . .venv/bin/activate && pip install -U pip && pip install -r requirements.txt
	@echo "Copy .env.example to .env and set OPENROUTER_* vars."

deps-lock:
	pip install pip-tools
	pip-compile requirements.in
	@echo "Dependencies locked in requirements.txt"

verify:
	. .venv/bin/activate && $(PY) scripts/verify.py

demo:
	. .venv/bin/activate && $(PY) scripts/demo.py

audit-pack:
	. .venv/bin/activate && $(PY) scripts/audit_pack.py

logs:
	. .venv/bin/activate && $(PY) scripts/make_logs.py

release: audit-pack logs
	. .venv/bin/activate && $(PY) scripts/make_release.py

validate:
	$(PY) scripts/test_roundtrip.py

fmt:
	black . && ruff check --fix .

demo-s1:
	. .venv/bin/activate && $(PY) orchestrator/skeleton.py --plan plans/plan-hello.json --state state/x-hello.json

demo-s1-llm:
	. .venv/bin/activate && $(PY) orchestrator/skeleton_llm.py --plan plans/plan-hello.json --state state/x-hello.json --llm kimi

demo-llm:
	. .venv/bin/activate && $(PY) orchestrator/skeleton_llm.py --plan plans/plan-hello.json --state state/x-hello.json --llm kimi --verifier local

demo-s1-mock:
	. .venv/bin/activate && $(PY) orchestrator/skeleton_llm.py --plan plans/plan-hello.json --state state/x-hello.json --llm mock

demo-s1-docker:
	. .venv/bin/activate && $(PY) orchestrator/skeleton_llm.py --plan plans/plan-hello.json --state state/x-hello.json --llm mock --verifier docker

demo-s2:
	. .venv/bin/activate && $(PY) scripts/metrics.py --tasks corpus/s2 --output artifacts/s2_metrics

delta-calibration:
	. .venv/bin/activate && jupyter nbconvert --to html notebook/delta_calibration.ipynb

test-rules:
	. .venv/bin/activate && $(PY) tests/test_rules.py

verifier-docker:
	. .venv/bin/activate && $(PY) scripts/verifier.py --runner docker --pcap examples/v0.1/pcap/ex1.json

docker-build:
	docker build -t proofengine/verifier:0.1.0 -f Dockerfile.verifier .

ci-local: verify demo audit-pack

# Supply-chain hardening targets
build-verifier-pinned:
	@echo "ðŸ” Verifying Docker image pin..."
	@grep -q "FROM python:3.11-slim@sha256:" Dockerfile.verifier || (echo "âŒ Docker image not pinned by digest" && exit 1)
	@echo "âœ… Docker image properly pinned"
	docker build -t proofengine/verifier:0.1.0 -f Dockerfile.verifier .

audit:
	@echo "ðŸ” Running security audit..."
	@echo "ðŸ“¦ Trivy filesystem scan..."
	trivy fs --exit-code 1 --severity HIGH,CRITICAL . || echo "âš ï¸ Trivy scan found issues"
	@echo "ðŸ³ Trivy image scan..."
	trivy image --exit-code 1 --severity HIGH,CRITICAL proofengine/verifier:0.1.0 || echo "âš ï¸ Trivy image scan found issues"
	@echo "ðŸ” Grype scan..."
	grype proofengine/verifier:0.1.0 --fail-on high || echo "âš ï¸ Grype scan found issues"
	@echo "ðŸ” Cosign verification..."
	@if [ -f "artifacts/audit_pack.zip" ]; then \
		cosign verify-blob artifacts/audit_pack.zip \
			--signature artifacts/audit_pack.zip.sig \
			--key .github/security/cosign.pub && echo "âœ… Cosign verification passed"; \
	else \
		echo "âš ï¸ No audit pack found for verification"; \
	fi
	@echo "âœ… Security audit completed"

# Expected-fail demonstrations
expected-fail-semver:
	@echo "ðŸ§ª Testing semver policy violation..."
	$(PY) scripts/verifier.py --runner docker --pcap examples/expected_fail/pcap-semver.json || echo "âœ… Expected failure: semver policy violation"

expected-fail-changelog:
	@echo "ðŸ§ª Testing changelog policy violation..."
	$(PY) scripts/verifier.py --runner docker --pcap examples/expected_fail/pcap-changelog.json || echo "âœ… Expected failure: changelog policy violation"

# Deterministic build test
rebuild-hash-equal:
	@echo "ðŸ”’ Testing deterministic build..."
	@export SOURCE_DATE_EPOCH=1700000000 && $(PY) scripts/build_audit_pack.py
	@sha256sum artifacts/audit_pack/audit_pack.zip > hash1.txt
	@export SOURCE_DATE_EPOCH=1700000000 && $(PY) scripts/build_audit_pack.py
	@sha256sum artifacts/audit_pack/audit_pack.zip > hash2.txt
	@if diff hash1.txt hash2.txt; then \
		echo "âœ… Deterministic build verified"; \
	else \
		echo "âŒ Build is not deterministic"; \
		exit 1; \
	fi
	@rm -f hash1.txt hash2.txt

# 2-Category transformation targets
2cat-shadow:
	@echo "ðŸ” Running 2-category shadow mode..."
	. .venv/bin/activate && $(PY) scripts/2cat_shadow_report.py
	@echo "âœ… Shadow report generated"

2cat-active:
	@echo "ðŸš€ Running 2-category active mode..."
	. .venv/bin/activate && $(PY) scripts/2cat_active_mode.py
	@echo "âœ… Active mode completed"

s2-bench:
	@echo "ðŸ“Š Running S2 benchmark..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py
	@echo "âœ… S2 benchmark completed"

2cat-report:
	@echo "ðŸ“ˆ Generating 2-category report..."
	. .venv/bin/activate && $(PY) scripts/generate_2cat_report.py
	@echo "âœ… 2-category report generated"

# Test 2-category strategies
test-2cat:
	@echo "ðŸ§ª Testing 2-category strategies..."
	. .venv/bin/activate && $(PY) -m pytest tests/strategies/ -v
	@echo "âœ… 2-category strategy tests completed"

# Expected-fail tests for 2-category
expected-fail-2cat:
	@echo "ðŸ§ª Testing 2-category expected-fail cases..."
	. .venv/bin/activate && $(PY) scripts/test_strategies_expected_fail.py
	@echo "âœ… 2-category expected-fail tests completed"

# S2 Vendors targets
demo-s2:
	@echo "ðŸŽ¯ Running S2 vendors demo..."
	. .venv/bin/activate && $(PY) scripts/bench_s2_vendors.py --mode both
	@echo "âœ… S2 vendors demo completed"

s2-active:
	@echo "ðŸš€ Running S2 active mode..."
	. .venv/bin/activate && $(PY) scripts/run_active_mock.py --plan corpus/s2/vendors/api-break/plan.json
	. .venv/bin/activate && $(PY) scripts/run_active_mock.py --plan corpus/s2/vendors/typosquat-cve/plan.json
	. .venv/bin/activate && $(PY) scripts/run_active_mock.py --plan corpus/s2/vendors/secret-egress/plan.json
	@echo "âœ… S2 active mode completed"

s2-bench:
	@echo "ðŸ“Š Running S2 vendors benchmark..."
	. .venv/bin/activate && $(PY) scripts/bench_s2_vendors.py --mode both --output artifacts/s2_vendors_benchmark.json
	@echo "âœ… S2 vendors benchmark completed"

# Expected-fail tests for S2 vendors
expected-fail-s2:
	@echo "ðŸ§ª Testing S2 vendors expected-fail cases..."
	. .venv/bin/activate && $(PY) -m pytest tests/s2_vendors_test.py -v
	@echo "âœ… S2 vendors expected-fail tests completed"

# Delta calibration and bench public targets
bench-baseline:
	@echo "ðŸ“Š Running baseline benchmark..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/bench_public/suite.json --mode baseline --runs 3 --out artifacts/bench_public/metrics_baseline.json
	@echo "âœ… Baseline benchmark completed"

bench-active:
	@echo "ðŸ“Š Running active benchmark..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/bench_public/suite.json --mode active --runs 3 --out artifacts/bench_public/metrics_active.json
	@echo "âœ… Active benchmark completed"

# Performance-optimized benchmarks
bench-baseline-fast:
	@echo "ðŸš€ Running baseline benchmark (optimized)..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/bench_public/suite.json --mode baseline --runs 3 --out artifacts/bench_public/metrics_baseline.json --parallel --workers 4 --compact-json --profile
	@echo "âœ… Baseline benchmark (optimized) completed"

bench-active-fast:
	@echo "ðŸš€ Running active benchmark (optimized)..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/bench_public/suite.json --mode active --runs 3 --out artifacts/bench_public/metrics_active.json --parallel --workers 4 --compact-json --profile
	@echo "âœ… Active benchmark (optimized) completed"

# Cache warmup
cache-warmup:
	@echo "ðŸ”¥ Warming up caches..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/bench_public/suite.json --mode baseline --runs 1 --out artifacts/bench_public/warmup.json --cache-warmup artifacts/cache_warmup.json
	@echo "âœ… Cache warmup completed"

delta-calibrate:
	@echo "ðŸ”§ Calibrating delta weights..."
	. .venv/bin/activate && $(PY) scripts/delta_calibrate.py --runs artifacts/bench_public/metrics_baseline.json artifacts/bench_public/metrics_active.json --out configs/weights.json --report artifacts/bench_public/delta_report.json
	@echo "âœ… Delta calibration completed"

repro-public:
	@echo "ðŸ”„ Running public reproduction..."
	bash scripts/repro_public.sh
	@echo "âœ… Public reproduction completed"

# S2++ targets
s2pp-shadow:
	@echo "ðŸ” Running S2++ shadow mode..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes baseline --runs 1 --out artifacts/s2pp/shadow
	@echo "âœ… S2++ shadow mode completed"

s2pp-active:
	@echo "ðŸš€ Running S2++ active mode..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes active --runs 1 --out artifacts/s2pp/active
	@echo "âœ… S2++ active mode completed"

s2pp-bench:
	@echo "ðŸ“Š Running S2++ benchmark..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes baseline,active --runs 3 --out artifacts/s2pp/bench
	@echo "âœ… S2++ benchmark completed"

s2pp-delta-calibrate:
	@echo "ðŸ”§ Calibrating S2++ delta weights..."
	. .venv/bin/activate && $(PY) scripts/delta_calibrate.py --input artifacts/s2pp/bench/metrics.csv --out configs/weights_v2.json --report artifacts/s2pp/delta_report.json --bootstrap 1000
	@echo "âœ… S2++ delta calibration completed"

repro-public-s2pp:
	@echo "ðŸ”„ Running S2++ public reproduction..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes baseline,active --runs 3 --out artifacts/s2pp/repro
	. .venv/bin/activate && $(PY) scripts/delta_calibrate.py --input artifacts/s2pp/repro/metrics.csv --out configs/weights_v2.json --report artifacts/s2pp/delta_report.json --bootstrap 1000
	@echo "ðŸ“¦ Creating audit pack..."
	. .venv/bin/activate && $(PY) scripts/audit_pack.py --output artifacts/s2pp/audit_pack.zip
	@echo "ðŸ” Signing audit pack..."
	cosign sign-blob --key .github/security/cosign.key artifacts/s2pp/audit_pack.zip --output artifacts/s2pp/audit.sig
	@echo "âœ… S2++ public reproduction completed"

# PR F1 Expected-fail targets
expected-fail-pii:
	@echo "ðŸ§ª Testing PII expected-fail cases..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes expected_fail --runs 1 --out artifacts/s2pp/expected_fail_pii --filter pii-logging
	@echo "âœ… PII expected-fail tests completed"

expected-fail-license:
	@echo "ðŸ§ª Testing License expected-fail cases..."
	. .venv/bin/activate && $(PY) scripts/bench_2cat.py --suite corpus/s2pp/suite.json --modes expected_fail --runs 1 --out artifacts/s2pp/expected_fail_license --filter license-violation-agpl
	@echo "âœ… License expected-fail tests completed"

# Architecture UnifiÃ©e v0.1 targets
arch-test:
	@echo "ðŸ§ª Testing Architecture UnifiÃ©e v0.1..."
	. .venv/bin/activate && $(PY) scripts/test_unified_architecture.py
	@echo "âœ… Architecture UnifiÃ©e v0.1 tests completed"

arch-demo:
	@echo "ðŸŽ¯ Running Architecture UnifiÃ©e v0.1 demo..."
	. .venv/bin/activate && $(PY) scripts/demo_unified_architecture.py
	@echo "âœ… Architecture UnifiÃ©e v0.1 demo completed"

arch-schemas:
	@echo "ðŸ“‹ Validating Architecture UnifiÃ©e v0.1 schemas..."
	. .venv/bin/activate && $(PY) scripts/test_roundtrip.py --schemas specs/v0.1/
	@echo "âœ… Architecture UnifiÃ©e v0.1 schemas validated"

arch-egraph:
	@echo "ðŸ”— Testing e-graph canonicalization..."
	. .venv/bin/activate && $(PY) -c "from proofengine.core.egraph import EGraph, canonicalize_state; egraph = EGraph(); print('E-graph initialized:', egraph.get_stats())"
	@echo "âœ… E-graph functionality verified"

arch-orchestrator:
	@echo "ðŸŽ­ Testing unified orchestrator..."
	. .venv/bin/activate && $(PY) -c "from proofengine.orchestrator.unified_orchestrator import UnifiedOrchestrator; print('Unified orchestrator imported successfully')"
	@echo "âœ… Unified orchestrator verified"

arch-full: arch-test arch-demo arch-schemas arch-egraph arch-orchestrator
	@echo "ðŸŽ‰ Architecture UnifiÃ©e v0.1 fully validated!"

# Discovery Engine 2-Cat specific targets
ae-test:
	@echo "ðŸ” Testing AE Next-Closure..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_ae_loop.py -v
	@echo "âœ… AE Next-Closure tests completed"

egraph-test:
	@echo "ðŸ”— Testing E-graph canonicalization..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_egraph.py -v
	@echo "âœ… E-graph tests completed"

bandit-test:
	@echo "ðŸŽ¯ Testing bandit/DPP selection..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_policy_selection.py -v
	@echo "âœ… Bandit/DPP tests completed"

ci-test:
	@echo "ðŸ”§ Testing CI components..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_ci_components.py -v
	@echo "âœ… CI tests completed"

discovery-test: ae-test egraph-test bandit-test ci-test
	@echo "ðŸŽ‰ Discovery Engine 2-Cat tests completed!"

discovery-demo:
	@echo "ðŸŽ¯ Running Discovery Engine 2-Cat demo..."
	. .venv/bin/activate && $(PY) scripts/demo_discovery_engine.py
	@echo "âœ… Discovery Engine 2-Cat demo completed"

# CI and attestation targets
ci-test:
	@echo "ðŸ”§ Testing CI components..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_ci_components.py -v
	@echo "âœ… CI tests completed"

hermetic-test:
	@echo "ðŸ”’ Testing hermetic runner..."
	. .venv/bin/activate && $(PY) runner/hermetic_stub.py
	@echo "âœ… Hermetic runner test completed"

merkle-test:
	@echo "ðŸ”— Testing Merkle journal..."
	. .venv/bin/activate && $(PY) scripts/merkle_journal.py
	@echo "âœ… Merkle journal test completed"

attestation:
	@echo "ðŸ” Generating attestation..."
	. .venv/bin/activate && $(PY) scripts/merkle_journal.py
	@echo "âœ… Attestation generated"

# Full CI pipeline
ci-full: ci-test hermetic-test merkle-test attestation
	@echo "ðŸŽ‰ Full CI pipeline completed!"

# RegTech demo targets
regtech-demo:
	@echo "ðŸŽ¯ Running RegTech Discovery Engine demo..."
	. .venv/bin/activate && $(PY) scripts/demo_regtech_bench.py
	@echo "âœ… RegTech demo completed"

regtech-test:
	@echo "ðŸ§ª Testing RegTech components..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_ae_loop.py -v
	@echo "âœ… RegTech tests completed"

# Complete Discovery Engine targets
discovery-full: discovery-test regtech-demo
	@echo "ðŸŽ‰ Complete Discovery Engine validation!"

# Incident handlers targets
incident-test:
	@echo "ðŸ”§ Testing incident handlers..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_incident_handlers.py -v
	@echo "âœ… Incident handlers tests completed"

# CI artifacts targets
ci-artifacts-test:
	@echo "ðŸ”§ Testing CI artifacts..."
	. .venv/bin/activate && $(PY) -m pytest tests/test_ci_components.py -v
	@echo "âœ… CI artifacts tests completed"

install-opa:
	@echo "ðŸ”§ Installing OPA..."
	bash scripts/install_opa.sh
	@echo "âœ… OPA installation completed"

# Complete CI pipeline
ci-complete: ci-test hermetic-test merkle-test incident-test ci-artifacts-test install-opa
	@echo "ðŸŽ‰ Complete CI pipeline with all components!"

# Final Discovery Engine validation
discovery-final: discovery-full incident-test ci-artifacts-test
	@echo "ðŸŽ‰ Complete Discovery Engine 2-Cat validation with all PRs!"

# Stabilization and release targets
determinism-test:
	@echo "ðŸ”¬ Running determinism test..."
	. .venv/bin/activate && $(PY) scripts/determinism_test.py --runs 3 --seed 42
	@echo "âœ… Determinism test completed"

bench-regtech:
	@echo "ðŸ“Š Running RegTech benchmark..."
	. .venv/bin/activate && $(PY) bench/run.py --suite regtech --baselines react,tot,dspy --ablations egraph,bandit,dpp,incident --out out/bench
	@echo "âœ… RegTech benchmark completed"

metrics-validation:
	@echo "ðŸ“ˆ Validating metrics..."
	. .venv/bin/activate && $(PY) -c "import json; metrics=json.load(open('out/metrics.json')); print('âœ… Metrics valid:', metrics.get('coverage', {}).get('accepted', 0) >= 3)"
	@echo "âœ… Metrics validation completed"

# Release preparation
release-prep: determinism-test bench-regtech metrics-validation
	@echo "ðŸŽ¯ Release preparation completed - ready for v0.1.0!"

# Docker and SBOM targets
docker-build:
	@echo "ðŸ³ Building Docker image..."
	docker build -t discovery-engine-2cat:v0.1.0 .
	@echo "âœ… Docker image built"

sbom-generate:
	@echo "ðŸ“¦ Generating SBOM..."
	curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin
	syft packages . -o spdx-json=out/sbom.json
	syft packages . -o table=out/sbom.txt
	@echo "âœ… SBOM generated"

security-scan:
	@echo "ðŸ” Running security scan..."
	curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin
	grype sbom:out/sbom.json -o table=out/vulnerabilities.txt
	grype sbom:out/sbom.json -o json=out/vulnerabilities.json
	@echo "âœ… Security scan completed"

# Complete release pipeline
release-pipeline: release-prep docker-build sbom-generate security-scan
	@echo "ðŸš€ Release pipeline completed - ready for v0.1.0 tag!"

# Top-Lab Readiness targets
bench:
	@echo "ðŸ“Š Running benchmark suite..."
	python bench/run.py --suite regtech --baselines react,tot,dspy --ablations egraph,bandit,dpp,incident --out out/bench
	@echo "âœ… Benchmark completed"

rollup:
	@echo "ðŸ“ˆ Computing metrics rollup..."
	python scripts/metrics_rollup.py out/ rollup/metrics-weekly.json
	@echo "âœ… Metrics rollup completed"

release:
	@echo "ðŸš€ Creating release..."
	gh workflow run Release -f version=$(VERSION)
	@echo "âœ… Release workflow triggered"

# Cursor Pack targets
fire-drill:
	@echo "ðŸ”¥ Running fire-drill Incidentâ†’Rule..."
	python scripts/fire_drill.py
	@echo "âœ… Fire-drill completed"

sweep-ids-cvar:
	@echo "ðŸ“Š Running IDS/CVaR parameter sweep..."
	python bench/run_sweep.py
	@echo "âœ… IDS/CVaR sweep completed"

discovery-final: ae-test egraph-test bandit-test incident-test ci-artifacts-test regtech-demo
	@echo "ðŸŽ‰ Complete Discovery Engine validation!"

ae-test:
	@echo "ðŸ§ª Testing AE components..."
	pytest -q -k "ae_loop or e2e_ae" || true
	@echo "âœ… AE tests completed"

egraph-test:
	@echo "ðŸ§ª Testing e-graph components..."
	pytest -q -k "egraph" || true
	@echo "âœ… E-graph tests completed"

bandit-test:
	@echo "ðŸ§ª Testing policy selection..."
	pytest -q -k "policy_selection" || true
	@echo "âœ… Bandit tests completed"

incident-test:
	@echo "ðŸ§ª Testing incident handlers..."
	pytest -q -k "incident_handlers" || true
	@echo "âœ… Incident tests completed"

ci-artifacts-test:
	@echo "ðŸ§ª Testing CI artifacts..."
	python scripts/merkle_journal.py orchestrator/journal/J.jsonl out/journal/merkle.txt || true
	@echo "âœ… CI artifacts tests completed"

regtech-demo:
	@echo "ðŸŽ¯ Running RegTech demo..."
	make demo || python scripts/demo_regtech_bench.py
	@echo "âœ… RegTech demo completed"

# Hardening v0.1.1 targets
determinism:
	@echo "ðŸ” Checking determinism..."
	python scripts/check_determinism.py
	@echo "âœ… Determinism check completed"

calibrate-budgets:
	@echo "ðŸ“Š Calibrating budgets and timeouts..."
	python scripts/calibrate_budgets.py
	@echo "âœ… Budget calibration completed"

# E-graph rules v0.2 targets
test-egraph-rules-v02:
	@echo "ðŸ”— Testing e-graph rules v0.2..."
	python -m pytest tests/test_egraph_rules_v02.py -v
	@echo "âœ… E-graph rules v0.2 tests completed"

# IDS/CVaR integration targets
test-policy-ids-cvar:
	@echo "ðŸ“Š Testing IDS/CVaR policy integration..."
	python -m pytest tests/test_policy_ids_cvar.py -v
	@echo "âœ… IDS/CVaR policy tests completed"

# HS-Tree integration targets
test-hstree:
	@echo "ðŸŒ³ Testing HS-Tree minimal test generation..."
	python -m pytest tests/test_hstree.py -v
	@echo "âœ… HS-Tree tests completed"

# Grove Pack targets
grove-pack:
	@echo "ðŸ“‹ Generating Grove pack..."
	python scripts/gen_onepager.py
	@echo "âœ… Grove pack generated"

# Site targets
site:
	@echo "ðŸŒ Building site..."
	mkdocs build
	@echo "âœ… Site built"

site-deploy:
	@echo "ðŸš€ Deploying site..."
	mkdocs gh-deploy --force
	@echo "âœ… Site deployed"

# Public benchmark pack targets
public-bench-pack:
	@echo "ðŸ“¦ Building public benchmark pack..."
	python scripts/build_public_bench_pack.py
	@echo "âœ… Public benchmark pack built"

# Complete hardening targets
hardening-v011: determinism test-egraph-rules-v02 calibrate-budgets test-policy-ids-cvar test-hstree
	@echo "ðŸŽ‰ Hardening v0.1.1 completed!"

# Complete Grove targets
grove-complete: grove-pack site public-bench-pack
	@echo "ðŸŽ‰ Grove pack completed!"

# All new targets
all-new: hardening-v011 grove-complete
	@echo "ðŸŽ‰ All new features completed!"

# T04: Summary validation
validate-summary:
	$(PY) -c "import json,sys; from pefc.metrics.validator import validate_summary_doc; from pathlib import Path; d=json.load(open('dist/summary.json')); validate_summary_doc(d, Path('schema/summary.schema.json'))" && echo "âœ… Summary validation passed"

# T17: CLI unifiÃ©e (Typer)
export PEFC_CONFIG ?= config/pack.yaml
export PIPELINE ?= config/pipelines/bench_pack.yaml

.PHONY: public-bench-pack verify-pack pack-manifest pack-sign

public-bench-pack:
	pefc --config $(PEFC_CONFIG) pack build --pipeline $(PIPELINE)

public-bench-pack-strict:
	pefc --config $(PEFC_CONFIG) pack build --pipeline $(PIPELINE) --strict

verify-pack:
	pefc pack verify --zip dist/*.zip --strict

pack-manifest:
	pefc pack manifest --zip dist/*.zip --print

pack-sign:
	pefc pack sign --in dist/*.zip --provider cosign

# Test targets
.PHONY: test test-cov test-fast test-slow test-all test-cli test-pipeline test-manifest test-metrics test-summary test-capabilities test-logging test-dedup

test:
	python -m pytest tests/ -v

test-cov:
	python -m pytest tests/ -v --cov=pefc --cov-report=html --cov-report=term

test-fast:
	python -m pytest tests/ -v -m "not slow"

test-slow:
	python -m pytest tests/ -v -m "slow"

test-all:
	python -m pytest tests/ -v --cov=pefc --cov-report=html --cov-report=term --cov-fail-under=80

test-cli:
	python -m pytest tests/test_cli_*.py -v

test-pipeline:
	python -m pytest tests/test_pipeline_*.py -v

test-manifest:
	python -m pytest tests/test_manifest_*.py -v

test-metrics:
	python -m pytest tests/test_metrics_*.py -v

test-summary:
	python -m pytest tests/test_summary_*.py -v

test-capabilities:
	python -m pytest tests/test_capabilities_*.py -v

test-logging:
	python -m pytest tests/test_logging_*.py -v

test-dedup:
	python -m pytest tests/test_zip_dedup_*.py -v

# New XME targets
.PHONY: dev lint test build sbom docker verify-2cat help

# Default target
help:
	@echo "Available targets:"
	@echo "  dev         - Enter development environment"
	@echo "  lint        - Run pre-commit hooks"
	@echo "  test        - Run pytest tests"
	@echo "  build       - Build the CLI using Nix"
	@echo "  sbom        - Generate Software Bill of Materials"
	@echo "  docker      - Build Docker image"
	@echo "  verify-2cat - Verify 2cat vendor package"
	@echo "  help        - Show this help message"

# Development environment
dev:
	nix develop

# Code quality
lint:
	pre-commit run -a

# Testing
test:
	pytest -q

# Build CLI
build:
	nix build .#xme

# Generate SBOM
sbom:
	mkdir -p sbom
	syft dir:. -o spdx-json > sbom/sbom.spdx.json

# Docker operations
docker:
	docker build -t xme/dev:latest .

# Vendor package verification
verify-2cat:
	bash scripts/verify_2cat_pack.sh

# PSP helpers
psp-schema:
	@xme psp schema --out docs/psp.schema.json
psp-normalize:
	@xme psp normalize examples/psp/mock_yoneda.json --out /tmp/psp_norm.json

# PCAP demo target
pcap:
	@RUN_PATH=$$(xme pcap new-run --out artifacts/pcap | awk '{print $$3}' | cut -d= -f2); \
	xme pcap log --run $$RUN_PATH --action "demo:start"; \
	xme pcap log --run $$RUN_PATH --action "demo:end"; \
	echo "RUN=$$RUN_PATH"; \
	xme pcap verify --run $$RUN_PATH; \
	xme pcap merkle --run $$RUN_PATH

# AE demo targets
ae-demo:
	@xme ae demo --context examples/fca/context_4x4.json --out artifacts/psp/ae_demo.json

ae-demo-4x4:
	@xme ae demo --context tests/fixtures/fca/context_4x4.json --out artifacts/psp/ae_4x4.json

ae-demo-5x3:
	@xme ae demo --context tests/fixtures/fca/context_5x3.json --out artifacts/psp/ae_5x3.json

# Golden files generation
goldens:
	@python scripts/gen_goldens.py